G:\Lenovo\hugggingface\qwenclient

Anaconda prompt中可以切换虚拟环境
# 下载大模型
~~~
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118  # GPU用户
pip install transformers huggingface_hub accelerate
~~~

## modeldownlad.py

下载模型到本地

~~~
from transformers import AutoModelForCausalLM, AutoTokenizer

# model_name = "Qwen/Qwen2.5-7B-Instruct"

model_name = "Qwen/Qwen3-4B-Instruct-2507"

# 下载 tokenizer

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

# 下载模型

model = AutoModelForCausalLM.from_pretrained(

    model_name,

    trust_remote_code=True,

    device_map="auto",  # GPU自动分配，CPU可设置为"cpu"

    low_cpu_mem_usage=True

)


# 保存到本地路径

# model.save_pretrained("./qwen_local")

# tokenizer.save_pretrained("./qwen_local")

model.save_pretrained("./qwen_local_4B")

tokenizer.save_pretrained("./qwen_local_4B")
~~~

# 在本地部署模型服务

为了方便 C# 调用，通常做法是：

1. 使用 **Python FastAPI** 启动本地 HTTP 接口。
    
2. C# 调用该 HTTP 接口。

## 安装Fast API

~~~
pip install fastapi uvicorn
~~~

## 创建API服务

qwen_server_gpu.py

~~~
from fastapi import FastAPI

from pydantic import BaseModel

# from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig

import torch

app = FastAPI(title="Qwen2.5-7B-Instruct GPU 本地服务")

model_name = "./qwen_local"  # 本地模型路径

# 量化配置

bnb_config = BitsAndBytesConfig(

    load_in_8bit=True,       # 使用 8-bit 量化

    # llm_int8_threshold=6.0,  # 可选参数，控制 int8 精度

    llm_int8_enable_fp32_cpu_offload=True  # CPU offload 开启

)

# 加载 tokenizer

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)


# 自定义 device_map，让部分模块放在 CPU 上

device_map = {

    "": "cuda",  # 默认放 GPU

}


# GPU 优化加载

try:

    model = AutoModelForCausalLM.from_pretrained(

        model_name,

        trust_remote_code=True,

        device_map="auto",  # 自动使用 GPU

        quantization_config=bnb_config,  # 新方式传入量化配置

    )

except Exception as e:

    print("模型加载失败:", e)

    exit(1)
 

class RequestBody(BaseModel):

    prompt: str

    max_tokens: int = 256

@app.post("/generate")

def generate(req: RequestBody):

    try:

        inputs = tokenizer(req.prompt, return_tensors="pt").to(model.device)

        generation_config = GenerationConfig(max_new_tokens=req.max_tokens)

        outputs = model.generate(**inputs, generation_config=generation_config)

        text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        return {"result": text}

    except Exception as e:

        # 返回 JSON，保证 C# 不报错

        return {"result": f"生成失败: {str(e)}"}

if __name__ == "__main__":

    import uvicorn

    print("启动 Qwen2.5-7B-Instruct GPU 本地服务...")

    uvicorn.run(app, host="127.0.0.1", port=8000)
~~~

# 在 C# 控制台调用模型

C# 通过 `HttpClient` 调用 Python API。

在C#项目中安装：

~~~
dotnet add package Xceed.Words.NET
dotnet add package itext7
~~~

- `Xceed.Words.NET`：读取 Word 文档。
    
- `itext7`：读取 PDF 文档。

~~~
using System.Text;
using System.Text.Json;
using Xceed.Words.NET; // DocX
using iText.Kernel.Pdf; // iText7 PDF

class Program
{
    static async Task Main()
    {
        Console.WriteLine("请输入要分析的文件路径（.txt/.pdf/.docx）：");
        string filePath = Console.ReadLine()?.Trim();
        if (!File.Exists(filePath))
        {
            Console.WriteLine("文件不存在！");
            return;
        }

        string text = "";
        string ext = Path.GetExtension(filePath).ToLower();

        if (ext == ".txt")
            text = await File.ReadAllTextAsync(filePath);
        else if (ext == ".pdf")
            text = ExtractTextFromPdf(filePath);
        else if (ext == ".docx")
            text = ExtractTextFromDocx(filePath);
        else
        {
            Console.WriteLine("不支持的文件类型！");
            return;
        }

        int maxCharsPerSegment = 256; // 每段最大字数，可根据模型显存调整
        var segments = SplitTextByLength(text, maxCharsPerSegment);

        Console.WriteLine($"文档共 {segments.Length} 段，开始总结...\n");

        StringBuilder finalSummary = new StringBuilder();
        int count = 1;
        foreach (var segment in segments)
        {
            string prompt = $"请对以下内容进行简明总结：\n{segment}";
            string summary = await GenerateTextFromModel(prompt);
            Console.WriteLine($"段落 {count} 总结：\n{summary}\n");
            finalSummary.AppendLine(summary);
            count++;
        }

        Console.WriteLine("=== 最终文档总结 ===");
        Console.WriteLine(finalSummary.ToString());
        Console.WriteLine("文档总结完成！");
    }

    // 按字数分段
    static string[] SplitTextByLength(string text, int maxLength)
    {
        var segments = new System.Collections.Generic.List<string>();
        int start = 0;
        while (start < text.Length)
        {
            int length = Math.Min(maxLength, text.Length - start);
            segments.Add(text.Substring(start, length));
            start += length;
        }
        return segments.ToArray();
    }

    static async Task<string> GenerateTextFromModel(string prompt)
    {
        using var client = new HttpClient { Timeout = TimeSpan.FromMinutes(10) };
        var request = new { prompt = prompt, max_tokens = 256 };
        string jsonRequest = JsonSerializer.Serialize(request);
        var content = new StringContent(jsonRequest, Encoding.UTF8, "application/json");

        try
        {
            var response = await client.PostAsync("http://127.0.0.1:8000/generate", content);
            string responseString = await response.Content.ReadAsStringAsync();
            using var doc = JsonDocument.Parse(responseString);
            return doc.RootElement.GetProperty("result").GetString() ?? "";
        }
        catch (TaskCanceledException)
        {
            return "调用模型超时，请尝试减少段落长度。";
        }
        catch (Exception ex)
        {
            return $"调用模型出错: {ex.Message}";
        }
    }

    static string ExtractTextFromPdf(string filePath)
    {
        var sb = new StringBuilder();
        using var reader = new PdfReader(filePath);
        using var pdfDoc = new PdfDocument(reader);
        for (int i = 1; i <= pdfDoc.GetNumberOfPages(); i++)
        {
            var page = pdfDoc.GetPage(i);
            sb.AppendLine(iText.Kernel.Pdf.Canvas.Parser.PdfTextExtractor.GetTextFromPage(page));
        }
        return sb.ToString();
    }

    static string ExtractTextFromDocx(string filePath)
    {
        using var doc = DocX.Load(filePath);
        return doc.Text;
    }
}
~~~

# 结果:

Qwen2.5-7B-Instruct + 8bit + CPU offload/每段最大字数为128，用了6min钟的时间
Qwen2.5-7B-Instruct + 8bit + CPU offload/每段最大字数为256，用了7min16s左右的时间分析第一段

# 测试Qwen/Qwen2.5-3B-Instruct模型结果

## 1
关闭CPU-offload，maxtokens=256分析的G:\Lenovo\hugggingface\Log_2023年5月7日.txt

第一段需38s，最终的总结仅仅是输出相应内容，没有分析

如果是对pdf形式的答辩程序总结，会胡言乱语

## 2
服务maxtokens:1000，同样的答辩程序文件，需150s左右，但几乎没有分析能力

## 3
~~~
            ┌────────────────────────────────────┐
            │            C# 客户端应用           │
            │------------------------------------│
原文段落 →  │ 多段并行请求 → SSE 流式接收总结... │
            └────────────────────────────────────┘
                           │REST API + SSE
                           ▼
            ┌────────────────────────────────────┐
            │           Python FastAPI            │
            │   分段任务队列 + 并行生成 + Streaming │
            └────────────────────────────────────┘
                           │
                           ▼
                   Qwen-3B/7B 模型
                   GPU + CPU offload
~~~

## 4
server.py

## 5

~~~
pip install fastapi uvicorn transformers torch sentencepiece python-docx PyMuPDF pdfplumber pydantic
pip install PySide6 requests

~~~

[[大模型Tkinter测试]]