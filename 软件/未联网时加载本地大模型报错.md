- 离线加载最关键的是：确保模型目录完整（config.json + tokenizer + 权重文件），并将该目录放到 loader 能搜索到的 cache_dir（我在 loader 中搜索了几种常见路径，包括 ModelScope 的默认缓存路径）。
- 我在 loader 中强制设置了 TRANSFORMERS_OFFLINE=1 / HF_OFFLINE=1，并使用 local_files_only=True 来阻止 transformers 发起网络请求。

加载模型的文件更换为model_loader_offline.py

The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

在调用 model.generate() 时传入了 temperature/top_p，但 transformers 在当前生成配置下并没有启用采样（do_sample=False 或者使用 beam search），因此这些采样相关参数会被忽略。解决办法是：明确启用采样（do_sample=True）或改用 beam search（num_beams>1，不使用 temperature/top_p），并在生成时只传入对当前策略有效的参数。

在模型加载器中（例如 model_loader_offline.py / model_loader_offline_verbose.py），把 generate 方法改成：自动判断是否使用采样并只在采样模式下传入 temperature/top_p/top_k；在非采样（greedy/beam）模式下使用 num_beams。

~~~
# （只展示 generate 方法及相关改动，放到你的 model_loader_offline.py 中替换原方法）
def generate(self, prompt: str, **gen_kwargs) -> str:
    """
    生成文本：自动在采样和 beam-search 之间切换，
    并且只在采样模式下传入 temperature/top_p/top_k 等参数，避免被忽略的警告。
    支持传入显式参数：
      - do_sample: True/False
      - temperature, top_p, top_k, num_beams, repetition_penalty, max_new_tokens
    """
    if self.model is None or self.tokenizer is None:
        raise RuntimeError("模型未加载，请先调用 load_model()")

    # 基本生成参数（优先使用 gen_kwargs，其次使用 MODEL_CONFIG）
    max_new_tokens = int(gen_kwargs.get("max_new_tokens", MODEL_CONFIG.get("max_new_tokens", 512)))
    repetition_penalty = float(gen_kwargs.get("repetition_penalty", MODEL_CONFIG.get("repetition_penalty", 1.15)))
    # user can explicitly pass do_sample; 如果未传则根据 temperature 决定
    do_sample = gen_kwargs.get("do_sample", None)
    temperature = gen_kwargs.get("temperature", MODEL_CONFIG.get("temperature", 0.0))
    top_p = gen_kwargs.get("top_p", MODEL_CONFIG.get("top_p", 1.0))
    top_k = gen_kwargs.get("top_k", MODEL_CONFIG.get("top_k", 50))
    num_beams = int(gen_kwargs.get("num_beams", MODEL_CONFIG.get("num_beams", 1)))

    # 决定是否采样：如果用户未显式传 do_sample，则当 temperature>0 且 top_p<1.0 或 top_k< (large) 时启用采样
    if do_sample is None:
        try:
            temp_f = float(temperature)
        except Exception:
            temp_f = 0.0
        # 有温度且温度>0 表示希望采样；否则使用 greedy/beam
        do_sample = (temp_f > 0.0)

    # prepare inputs
    inputs = self.tokenizer(prompt, return_tensors="pt", padding=True)
    device = self.device if self.device else ("cuda" if torch.cuda.is_available() else "cpu")
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # build generation kwargs and only include sampling args when do_sample=True
    gen_args = {
        "max_new_tokens": max_new_tokens,
        "repetition_penalty": repetition_penalty,
        "pad_token_id": self.tokenizer.pad_token_id,
        "eos_token_id": self.tokenizer.eos_token_id,
        "do_sample": bool(do_sample),
    }

    if do_sample:
        # 采样模式：这些参数会生效
        gen_args.update({
            "temperature": float(temperature),
            "top_p": float(top_p),
            "top_k": int(top_k),
        })
        # 你也可以允许用户传入 `top_k`/`top_p`/`temperature` via gen_kwargs
    else:
        # 非采样（greedy/beam）：使用 beam search 参数
        # 如果 num_beams>1，则启用 beam search
        gen_args.update({
            "num_beams": int(num_beams),
            "early_stopping": True if int(num_beams) > 1 else False,
        })

    # 生成
    with torch.no_grad():
        outputs = self.model.generate(
            **inputs,
            **gen_args,
        )

    input_len = inputs["input_ids"].shape[1]
    generated = outputs[0][input_len:]
    return self.tokenizer.decode(generated, skip_special_tokens=True)
~~~

INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).

accelerate 在提醒你：它为 device_map="auto" 时自动给每个设备分配内存比例（默认 90% 用于模型，10% 作为缓冲区），只是信息级别提示，不是错误。它并不会阻止模型加载，但说明你可以显式设置 max_memory 或采用其它内存控制手段来更精确地分配显存/内存，或避免 OOM。

我给你一个实用的改进版 model_loader_offline.py（只替换加载时的部分），它会：

- 在 CUDA 情况下自动计算 GPU 总显存并生成合适的 max_memory 字典传给 from_pretrained；
- 使用 low_cpu_mem_usage=True（减少启动时 CPU 内存峰值）；
- 根据配置优先使用 fp16 或 8bit（如果 bitsandbytes 可用）；
- 并打印最终传入 from_pretrained 的关键参数，便于调试。
请把下面文件保存为 model_loader_offline.py（替换你当前的同名文件），然后重启服务观察日志。

~~~
"""离线优先的模型加载器 - 带显存自动分配 (max_memory) 支持"""

import os
from pathlib import Path
from typing import Optional
import logging
import torch

# 强制离线模式，防止 transformers 访问网络
os.environ.setdefault("TRANSFORMERS_OFFLINE", "1")
os.environ.setdefault("HF_OFFLINE", "1")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# 导入配置
try:
    from config_modelscope import MODEL_CONFIG
except Exception:
    from config import MODEL_CONFIG

# 基础检查项与权重后缀
_REQUIRED_MODEL_FILES = ["config.json", "tokenizer.json"]
_MODEL_WEIGHT_EXTS = ("*.safetensors", "*.bin")


def find_local_model_dir(cache_dir: Path, model_name: str) -> Optional[Path]:
    cache_dir = Path(cache_dir).expanduser().resolve()
    if not cache_dir.exists():
        return None
    owner, _, repo = model_name.partition("/")
    repo_short = repo.replace(".", "_")
    candidates = []
    search_roots = [cache_dir, cache_dir / "hub" / "models", cache_dir / "models", cache_dir / "hub/models"]
    for root in search_roots:
        if not root.exists():
            continue
        for p in root.rglob("*"):
            if not p.is_dir():
                continue
            whole = str(p).lower()
            name = p.name.lower()
            if owner.lower() in whole and repo_short.lower() in name:
                candidates.append(p)
            elif repo_short.lower() in name:
                candidates.append(p)
    uniq = []
    for c in candidates:
        if c not in uniq:
            uniq.append(c)
    uniq.sort(key=lambda x: len(str(x)))
    for c in uniq:
        ok = True
        for f in _REQUIRED_MODEL_FILES:
            if not (c / f).exists():
                ok = False
                break
        has_weights = any(list(c.rglob(ext)) for ext in _MODEL_WEIGHT_EXTS)
        if ok and has_weights:
            return c
    return None


class OfflineModelLoader:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = None

    def _get_device(self) -> str:
        device = MODEL_CONFIG.get("device", "auto")
        if device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                return "mps"
            else:
                return "cpu"
        return device

    def _build_max_memory(self, device: str):
        """
        为 device_map='auto' 生成 max_memory 字典：
        - 对单 GPU：将 GPU 总显存的 90% 用于模型，其余给 CPU（示例）
        - 对多 GPU/复杂环境可以扩展
        返回 None 或 dict（符合 accelerate/transformers 要求）
        """
        if device != "cuda":
            return None
        try:
            # 仅示例：使用第0块 GPU 的总显存
            total_bytes = torch.cuda.get_device_properties(0).total_memory
            total_gb = total_bytes / (1024 ** 3)
            # 预留 10% 给缓冲，防止 OOM；至少保留 1GB
            alloc_gb = max(1, int(total_gb * 0.9))
            # 设置 CPU 作为备用（经验值，按需调整）
            cpu_gb = max(8, int((total_gb * 0.1) + 8))
            max_memory = {0: f"{alloc_gb}GB", "cpu": f"{cpu_gb}GB"}
            logger.info(f"自动计算 max_memory: {max_memory} (GPU 总显存约 {total_gb:.1f}GB)")
            return max_memory
        except Exception as e:
            logger.warning(f"无法获取 GPU 信息以自动计算 max_memory: {e}")
            return None

    def load_model(self, model_path: Optional[str] = None):
        # 延迟导入 transformers，确保离线 env 生效
        import transformers  # noqa: F401
        from transformers import AutoTokenizer, AutoModelForCausalLM

        self.device = self._get_device()
        logger.info(f"目标设备: {self.device}")

        if model_path:
            local_dir = Path(model_path).expanduser().resolve()
            if not local_dir.exists():
                raise FileNotFoundError(f"指定的本地模型路径不存在: {local_dir}")
        else:
            model_name = MODEL_CONFIG.get("model_name")
            cache_dir_conf = MODEL_CONFIG.get("cache_dir")
            possible_cache_dirs = []
            if cache_dir_conf:
                possible_cache_dirs.append(Path(cache_dir_conf))
            possible_cache_dirs.append(Path.home() / ".cache" / "modelscope" / "hub")
            possible_cache_dirs.append(Path.home() / ".cache" / "huggingface" / "hub")
            possible_cache_dirs.append(Path.home() / ".cache" / "huggingface")

            found = None
            for cdir in possible_cache_dirs:
                if not cdir:
                    continue
                logger.debug(f"在缓存目录中查找: {cdir}")
                cand = find_local_model_dir(cdir, model_name)
                if cand:
                    found = cand
                    logger.info(f"找到本地模型目录: {found}")
                    break

            if not found:
                raise RuntimeError(
                    "未能在本地缓存中找到模型。\n"
                    "请在有网络的机器上运行 download_model_online.py 下载模型并将目录复制到离线机器。\n"
                    "示例目标目录（Windows）: C:\\Users\\<user>\\.cache\\modelscope\\hub\\models\\Qwen\\<model-dir>"
                )
            local_dir = found

        # 确保 transformers 离线
        os.environ["TRANSFORMERS_OFFLINE"] = "1"
        os.environ["HF_OFFLINE"] = "1"

        logger.info(f"使用本地模型路径加载: {local_dir}")

        # 判断是否启用 bitsandbytes 量化（如果安装了 bitsandbytes 且配置允许）
        use_8bit = False
        try:
            import bitsandbytes  # type: ignore
            if MODEL_CONFIG.get("use_8bit", False):
                use_8bit = True
                logger.info("检测到 bitsandbytes，将尝试以 8-bit 加载（如果配置启用）")
        except Exception:
            pass

        # 准备 load_kwargs
        load_kwargs = {
            "local_files_only": True,
            "trust_remote_code": True,
            # 降低 CPU 内存峰值
            "low_cpu_mem_usage": True,
        }

        # 设置精度与 device_map
        if MODEL_CONFIG.get("use_fp16", False) and self.device in ("cuda", "mps"):
            load_kwargs["torch_dtype"] = torch.float16

        if use_8bit:
            # transformers 侧不一定直接支持 load_in_8bit 参数；通常通过 accelerate + bitsandbytes，client侧需确保支持
            load_kwargs["load_in_8bit"] = True

        if self.device == "cuda":
            load_kwargs["device_map"] = "auto"
            # 自动计算 max_memory 并传入（避免 accelerate 自动选择 90% 的提示）
            max_mem = self._build_max_memory(self.device)
            if max_mem:
                load_kwargs["max_memory"] = max_mem

        # 打印 debug 信息，便于确认
        logger.info(f"from_pretrained 调用参数示例: device_map={load_kwargs.get('device_map')}, "
                    f"max_memory={load_kwargs.get('max_memory')}, torch_dtype={load_kwargs.get('torch_dtype', None)}, "
                    f"low_cpu_mem_usage={load_kwargs.get('low_cpu_mem_usage', None)}, load_in_8bit={load_kwargs.get('load_in_8bit', None)}")

        # 加载 tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            str(local_dir),
            local_files_only=True,
            trust_remote_code=True,
        )
        # 确保 pad_token 存在
        if getattr(self.tokenizer, "pad_token", None) is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # 加载模型
        self.model = AutoModelForCausalLM.from_pretrained(str(local_dir), **load_kwargs)

        # 如果没有 device_map，手动移动到设备（某些自动装载方式已完成分配）
        if self.device != "cuda":
            try:
                self.model = self.model.to(self.device)
            except Exception:
                pass

        self.model.eval()
        logger.info("本地模型加载完成（离线）")

    def generate(self, prompt: str, **gen_kwargs) -> str:
        if self.model is None or self.tokenizer is None:
            raise RuntimeError("模型未加载，请先调用 load_model()")
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True)
        device = self.device if self.device else ("cuda" if torch.cuda.is_available() else "cpu")
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # 生成参数（同之前逻辑）
        max_new_tokens = int(gen_kwargs.get("max_new_tokens", MODEL_CONFIG.get("max_new_tokens", 512)))
        repetition_penalty = float(gen_kwargs.get("repetition_penalty", MODEL_CONFIG.get("repetition_penalty", 1.15)))
        do_sample = gen_kwargs.get("do_sample", None)
        temperature = gen_kwargs.get("temperature", MODEL_CONFIG.get("temperature", 0.0))
        top_p = gen_kwargs.get("top_p", MODEL_CONFIG.get("top_p", 1.0))
        top_k = gen_kwargs.get("top_k", MODEL_CONFIG.get("top_k", 50))
        num_beams = int(gen_kwargs.get("num_beams", MODEL_CONFIG.get("num_beams", 1)))

        if do_sample is None:
            try:
                temp_f = float(temperature)
            except Exception:
                temp_f = 0.0
            do_sample = (temp_f > 0.0)

        gen_args = {
            "max_new_tokens": max_new_tokens,
            "repetition_penalty": repetition_penalty,
            "pad_token_id": self.tokenizer.pad_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "do_sample": bool(do_sample),
        }

        if do_sample:
            gen_args.update({
                "temperature": float(temperature),
                "top_p": float(top_p),
                "top_k": int(top_k),
            })
        else:
            gen_args.update({
                "num_beams": int(num_beams),
                "early_stopping": True if int(num_beams) > 1 else False,
            })

        logger.debug(f"model.generate kwargs: {gen_args}")

        with torch.no_grad():
            outputs = self.model.generate(**inputs, **gen_args)

        input_len = inputs["input_ids"].shape[1]
        generated = outputs[0][input_len:]
        return self.tokenizer.decode(generated, skip_special_tokens=True)


# 全局实例
model_loader = OfflineModelLoader()
~~~
