[[1706.03762] Attention Is All You Need](https://arxiv.org/abs/1706.03762)
## 神经网络结构搜索

Neural Architecture Search（NAS）：找到最佳的网络结构使validation accuracy最高

超参数：神经网络结构/算法
参数：权重等

以CNN为例
1. 事先制定搜索空间，

![[Pasted image 20251029110500.png]]

2. 基础的随即搜索方法：Baseline

随即在可能的超参数中采用，训练网络得到结果，选择结果最好的为最佳结果

3. 问题：搜索空间巨大/每次尝试耗时较长

## NAS via RNN + RL

用RNN生成网络结构，并且用RL训练RNN--目前已无人用

论文：RNN for generating CNN Architectures, Zoph & Le ,ICILR, 2017

用RNN生成CNN结构

![[Pasted image 20251029113210.png]]

![[Pasted image 20251029113426.png]]

![[Pasted image 20251029113549.png]]
是否可微

![[Pasted image 20251029115010.png]]

只能用强化学习，因为强化学习不要求最大化的目标是可微函数
强化学习并不好，既然用了强化学习，就得付出很大代价，需要做大量的计算去收集奖励。

![[Pasted image 20251029120126.png]]
策略梯度上升更新函数

![[Pasted image 20251029120305.png]]

example
![[Pasted image 20251029120412.png]]

## Differentiable NAS

DARTS
FBNet

![[Pasted image 20251029133242.png]]

latency尽量小，网络预测尽量快
通过latency以及loss来选择最佳的网络结构

不同的设备适用的网络结构不 同
![[Pasted image 20251029134451.png]]

## 并行计算与机器学习

cpu时间
gpu时间：20个gpu计算一天，gpu时间为20天
钟表时间：并行计算可以减少，但不会减少上两个

线性回归
![[Pasted image 20251029134954.png]]

![[Pasted image 20251029135202.png]]

![[Pasted image 20251029135537.png]]

一半的样本交给第一个处理器，剩下的交给第二个处理器
![[Pasted image 20251029135657.png]]

![[Pasted image 20251029135759.png]]

# 数据处理基础

one-hot encoding，从1开始，因为0可以代表缺失的特征

![[Pasted image 20251029141536.png]]

![[Pasted image 20251029141659.png]]

处理text data

![[Pasted image 20251029141832.png]]

计算词频，hash表
![[Pasted image 20251029141935.png]]

![[Pasted image 20251029142006.png]]

![[Pasted image 20251029142108.png]]

![[Pasted image 20251029142146.png]]

字典里单词的个数称为vocabulary词汇量
![[Pasted image 20251029142215.png]]

![[Pasted image 20251029142454.png]]

![[Pasted image 20251029142542.png]]

![[Pasted image 20251029142619.png]]

### 处理器之间的通信

共享内存：一个处理器可以直接看到其他处理器的结果，没办法大规模并行
消息传递：多个节点，每个节点有几个处理器，节点内的处理器可以共享内存，节点之间可以用网线也可以远程用TCP/IP发消息，

![[Pasted image 20251029140043.png]]

worker计算，server协调
![[Pasted image 20251029140241.png]]

![[Pasted image 20251029140407.png]]

# 文本处理和word Embedding

## text to sequence

![[Pasted image 20251029144941.png]]

![[Pasted image 20251029145041.png]]

![[Pasted image 20251029145105.png]]

![[Pasted image 20251029145148.png]]

每条序列须有相同的长度

![[Pasted image 20251029145257.png]]
![[Pasted image 20251029145332.png]]

## word embedding : word to vector

![[Pasted image 20251029145515.png]]

![[Pasted image 20251029145618.png]]

![[Pasted image 20251029145739.png]]

sequence长度：word_num（选择每条评论的后20个），词向量维度：embedding_dim（通过交叉验证选出）
embedding层输出：每个电影评论中有20个单词，每个单词用8维的词向量来表示
![[Pasted image 20251029145944.png]]

## 逻辑回归进行二分类

![[Pasted image 20251029150503.png]]

![[Pasted image 20251029150700.png]]

## 总结

![[Pasted image 20251029150902.png]]
![[Pasted image 20251029150934.png]]

![[Pasted image 20251029151057.png]]


# RNN

小规模上比较有用，大规模transformer等更好

simple RNN: ，没有激活函数会导致数值计算趋于0或是爆炸
![[Pasted image 20251030121126.png]]

![[Pasted image 20251030121335.png]]



# LSTM



# Attention



# Transformer

