# 注意

为了SVAR计算，z2层的神经元数量必须要按照从前的设置，否则会出问题

# 孪生代码

~~~
import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

from scipy.interpolate import make_interp_spline

from sklearn.preprocessing import MinMaxScaler

from sklearn.metrics import r2_score

import torch

  

class BP:

    def __init__(self, batchsize, lags, features, learning_rate):

        # 每次输入训练样本数量

        self.samples=batchsize

        # 滞后

        self.lags=lags

        # 样本特征数

        self.feautures=features

        self.num = self.samples

        # 网络输出神经元节点数量

        self.outnum = self.feautures * self.feautures * (self.lags+1)

        # 轴承

        # self.outnum = 4 * (self.lags+1) 这样写计算svar的时候会有问题

  

        self.hidnum = self.feautures * self.feautures

  

        # self.hidnum = int(self.feautures / 2)

        self.input = np.zeros((self.num, self.feautures))  # （1，features)

        self.w1 = 2 * np.random.random((self.feautures, self.hidnum)) - 1  # limit to (-1, 1)

        self.z1 = 2 * np.random.random((self.num, self.hidnum)) - 1

        self.hidden_layer_1 = np.zeros((self.num, self.hidnum))

        self.w2 = 2 * np.random.random((self.hidnum, self.outnum)) - 1

        self.z2 = 2 * np.random.random((self.num, self.outnum)) - 1

        # 仿真输入

        self.h = np.zeros((self.lags + batchsize,self.feautures))

        # 仿真输出

        self.output_layer = np.zeros((self.num, self.feautures))

        self.label = np.zeros((self.num, self.feautures))

        # self.result = np.zeros((self.num,self.feautures))

        self.loss = np.zeros((self.num, self.feautures))

        self.learning_rate = learning_rate

    def tanh(self,x):

        return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))

    def tanh_deri(self,x):

        t=(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))

        return 1-t*t

    def relu(self,x):

        return np.maximum(0,x)

    def relu_deri(self,x):

        return np.where(x<0,0,1)

    def sigmoid(self, x):

  

        return 1 / (1 + np.exp(-x))

  

    def sigmoid_deri(self, x):

  

        return x * (1 - x)

  

    def forward_prop(self, data, h):  

        self.input = data

        self.h = h

        self.z1 = np.dot(self.input, self.w1)

        self.hidden_layer_1 = self.sigmoid(self.z1)

        self.z2 = np.dot(self.hidden_layer_1, self.w2)

        # 不是分类问题，不采用激活函数

        self.output_layer = self.svar()

        # error

        self.label = self.input

        self.loss = 1/2 * (self.output_layer - self.label) ** 2

        # self.loss = 1/2 * ((self.output_layer - self.label) ** 2 +0.1*self._dag_constraint())

  

        return self.output_layer,self.z2

    def svar(self):

        X = self.h.T

        n_features = X.shape[0]

        n_samples = X.shape[1]

        matrix_num = self.feautures * self.feautures

        # 轴承

        # matrix_num = 4

        M_iters = self.lags + 1

        # M_taus = np.zeros((M_iters, self.feautures, self.feautures))

        # for i in range(M_iters):

        #     a = i * matrix_num

        #     b = (i + 1) * matrix_num

        #     M_taus[i] = self.z2[0,a:b].reshape((self.feautures,self.feautures))

            # M_taus这样写有问题，在多个samples的时候

        result = np.zeros((n_features, n_samples))

        for t in range(n_samples):

            if t - self.lags < 0:

                continue

            M_taus = np.zeros((M_iters, self.feautures, self.feautures))

            # M_taus = np.zeros((M_iters, 2, 2))

            for i in range(M_iters):

                a = i * matrix_num

                b = (i + 1) * matrix_num

                M_taus[i] = self.z2[t - lags,a:b].reshape((self.feautures,self.feautures))

                # M_taus[i] = self.z2[t - lags,a:b].reshape((2,2))

  

            # estimated = np.zeros((n_features, 1))

            estimated = np.zeros((n_features, 1))

            for tau in range(0, self.lags + 1):

                mid = X[:, t - tau].reshape((-1, 1))

                estimated += np.dot(M_taus[tau],mid)

            result[:, t] = estimated.reshape((-1,))

            # residuals[:, t] = X[:, t] - estimated.reshape((-1,))

  

        # residuals = residuals[:, self.lags:].T

        result = result[:, self.lags:].T

        # self.loss = 1/2 * (self.input - result) ** 2

  

        return result

    def _dag_constraint(self):

        """ 计算DAG无环性约束损失（NOTEARS方法） """

        B = self.z2.reshape(-1,self.feautures)

        b=np.zeros((self.lags,self.feautures,self.feautures))

        m=self.feautures*2

        B_1=B[self.feautures:m,:]

        B_2=B[m:,:]

        B[0]=B_1

        B[1]=B_2

        M1 = torch.matmul(B_1, B_1)

        M2 = torch.matmul(B_2, B_2)

        h1 = torch.trace(torch.matrix_exp(M1)) - self.n

        h2 = torch.trace(torch.matrix_exp(M2)) - self.n

        return h1+h2

  

    def oderivz(self):

        n_samples = self.h.shape[0]

        m = self.feautures * self.feautures

        der = np.zeros((n_samples, m, 1))

        for i in range(n_samples):

            x = self.h[i,:].reshape(-1,1)

            der[i] = np.tile(x,(self.feautures, 1))

            if i == n_samples-1:

                for k in range(self.feautures):

                    der[i,k*(self.feautures+1),0]=der[i,k*(self.feautures+1),0]-1

        o_derivz = np.zeros((self.num, self.outnum))

        om = self.lags + 1

        for i in range(self.num):

            ox = np.zeros((m*om, 1))

            for j in range(om, 0, -1):

                lefth = (om - j) * m

                righth  = (om - j + 1) * m

                ox[lefth:righth, 0] = der[i+j-1].reshape(m, )

            o_derivz[i,:] = ox.transpose()

  

        return o_derivz

  

    def func_lout(self,x):

        # 将l_deri_out重新改形式，以便实现l_deri_z2

        # m = (self.feautures * self.feautures) * (self.lags + 1)

        arrx = np.zeros((x.shape[0], self.outnum))

        midx = np.zeros((x.shape[0],self.feautures * self.feautures))

        for i in range(x.shape[0]):

            for j in range(self.feautures):

                a = j*self.feautures

                b = (j + 1) * self.feautures

                midx[i, a:b] = x[i,j]

            arrx[i,:] = np.tile(midx[i, :], (1,self.lags + 1))

        return arrx

  

    def backward_prop(self):

        # w2

        l_deri_out = self.output_layer - self.label

        # l_deri_out = np.tile(l_deri_out,(1,12))

        o_deri_z2 = self.oderivz()

        # 矩阵元素点乘

        # l_deri_z2 = l_deri_out * self.sigmoid_deri(self.output_layer)

        l_deri_out_1 = self.func_lout(l_deri_out)

        l_deri_z2 = l_deri_out_1 * o_deri_z2

        # 矩阵乘法

        l_deri_w2 = np.dot(self.hidden_layer_1.T, l_deri_z2)

        # w1

        l_deri_h1 = np.dot(l_deri_z2, self.w2.T)

        l_deri_z1 = l_deri_h1 * self.sigmoid_deri(self.hidden_layer_1)

        l_deri_w1 = np.dot(self.input.T, l_deri_z1)

        self.w2 -= self.learning_rate * l_deri_w2

        self.w1 -= self.learning_rate * l_deri_w1

  

def load_data(a, b):

    global MAX, MIN

    global mm

    # df = pd.read_csv('dataF106.csv', encoding='gbk')

  

    df = pd.read_csv('46.csv', encoding='gbk')

    # df = df.iloc[:300,:]

    df = df.iloc[a:b,:]

  

    # df.drop(['Motor_T2', 'MB_T2', 'Bearing_T3'], axis=1, inplace=True)

    # df = df.diff().dropna()

    # df.drop(['Motor_T2', 'MB_T2', 'Bearing_T3','Hz','RPM'], axis=1, inplace=True)

    df = df.to_numpy()

    # train_x = df

    # train_y = df

    # test_x = df

    # test_y = df

    # df_s = pd.DataFrame(df)

    # df_s.to_csv('df.csv')

    # 按列归一化

    df_t = mm.fit_transform(df)

    # df_t=df

    train_x = df_t

    train_y = df_t

    test_x = df_t

    # test_y = test_x[lags:,:]

    test_y = df_t

  

    return train_x, train_y ,test_x ,test_y

  

def train(train_x, batch_size, epochs,lags):

    # print('training...')

    batch = int((len(train_x) - lags) / batch_size)

    # train_x[lags:lags+batch*batch_size：]

    for epoch in range(epochs):

        total_loss = 0

        for i in range(batch):

            start = lags + i * batch_size

            end = start + batch_size

            # if end > len(train_x):

            #     end = len(train_x)

            #     start = end - batch_size

            nn.forward_prop(train_x[start:end], train_x[start - lags:end])

            nn.backward_prop()

            total_loss += nn.loss

            # train_loss.append(np.mean(nn.loss))

            # print('当前batch:', i+1, ' error:', np.mean(nn.loss))

        # print('当前epoch:', epoch, ' error:', np.mean(nn.loss))

        # print('当前epoch:', epoch, ' error:', np.mean(train_loss))

        # train_loss_epoch.append(np.mean(nn.loss))

        # train_loss_epoch.append(np.mean(train_loss))

    # return nn, train_loss, train_loss_epoch

        train_loss_epoch.append(total_loss / batch)

    return nn, train_loss_epoch

  

def get_mape(x, y):

    """

    :param x:真实值

    :param y:预测值

    :return:MAPE

    """

    return np.mean(np.abs((x - y) / x))

  

def get_r2(x,y):

    return r2_score(y_true=x, y_pred=y)

  

def get_adjustr2(x,y):

    r2=r2_score(y_true=x, y_pred=y)

    [n,p]=r2.shape

    adjust_r2=1 - ((1 - r2) * (n - 1)) / (n - p - 1)

    return adjust_r2

  

def test(test_x, test_y, batch_size,lags):

    global mm

    batch = int((len(test_x) - lags) / batch_size)

    pred = np.zeros((test_x.shape[0],test_x.shape[1]))

    for i in range(batch):

        start = lags + i * batch_size

        end = start + batch_size

        # if end > len(test_x):

        #     end = len(test_x)

        #     start = end - batch_size

        res,z2=nn.forward_prop(test_x[start:end], test_x[start - lags:end])

        pred[start:end,:]=res

    pred = pred[lags:lags + batch * batch_size,:]

    test_y = test_y[lags:lags + batch * batch_size,:]

    test_y = mm.inverse_transform(test_y)

    pred = mm.inverse_transform(pred)

    # pred = pd.DataFrame(pred)

    # test_y = pd.DataFrame(test_y)

    # # ***********************

    # pred.to_csv("pred_300.csv")

    # test_y.to_csv("test_y.csv")

  

    accuracy = get_mape(test_y, pred)

    print('accuracy:', accuracy)

    plt.plot(pred[:,5], c='red', marker='o', ms=1, alpha=0.75, label='pred')

    plt.plot(test_y[:,5], c='blue', marker='*', ms=1, alpha=0.75, label='real')

    plt.grid(axis='y')

    plt.legend()

    plt.show()

  

    return accuracy,z2

  

def test_online(test_x, lags):

    global mm

    res,z2=nn.forward_prop(test_x[lags:,:],test_x)

    test_y = test_x

    test_y = mm.inverse_transform(test_y)

    res = mm.inverse_transform(res)

    test_y = test_y[lags:,:]

    # 6个参数

    # accuracy = get_mape(test_y[-1,5], res[-1,5])

    # 9个参数

    # accuracy = get_mape(test_y[-1,7], res[-1,7])

  

    # 参数同时计算

    col=test_x.shape[1]

    accuracy=np.zeros(col)

    r2=np.zeros(col)

    for i in range(col):

        accuracy[i]=get_mape(test_y[-1,i], res[-1,i])

        r2[i]=get_r2(test_y[:,i], res[:,i])

  

    return accuracy, res, test_y,r2,z2

  

def Save_to_Csv(data, file_name, Save_format = 'csv', Save_type = 'col'):

    # data

    # 输入为一个字典，格式： { '列名称': 数据,....}

    # 列名即为CSV中数据对应的列名， 数据为一个列表

    # file_name 存储文件的名字

    # Save_format 为存储类型， 默认csv格式， 可改为 excel

    # Save_type 存储类型 默认按列存储， 否则按行存储

    # 默认存储在当前路径下

    Name = []

    times = 0

    if Save_type == 'col':

        for name, List in data.items():

            Name.append(name)

            if times == 0:

                Data = np.array(List).reshape(-1,1)

            else:

                Data = np.hstack((Data, np.array(List).reshape(-1,1)))

            times += 1

        Pd_data = pd.DataFrame(columns=Name, data=Data)

    else:

        for name, List in data.items():

            Name.append(name)

            if times == 0:

                Data = np.array(List)

            else:

                Data = np.vstack((Data, np.array(List)))

            times += 1

        Pd_data = pd.DataFrame(index=Name, data=Data)  

    if Save_format == 'csv':

        Pd_data.to_csv('./'+ file_name +'.csv',encoding='utf-8')

    else:

        Pd_data.to_excel('./'+ file_name +'.xls',encoding='utf-8')

  

# MAX, MIN = 0, 0

mm = MinMaxScaler()

import time

  

if __name__ == '__main__':

    # np.random.seed(42)

    # 这个batchsize和lags好像比较好

    batchsize = 2

    lags = 2

    # features = 6

    features = 8

    learning_rate = 0.05

    epochs = 50

    # a = 45750

    # # b = 46250

    # b = 46350

  

    # a = 20000

    # b = 20186

  

    a = 1

    b = 604

    # a=43000

    # b=43500

    # a=44000

    # b=46000

    train_size = 600

    train_loss=[]

    train_loss_epoch = []

    nn = BP(batchsize = batchsize, lags = lags, features = features, learning_rate = learning_rate)

    train_x,train_y,test_x,test_y = load_data(a=a, b=b)

    # 多少个batchsize就是一次性预测接下来几个，此话不严谨，若是实时的，最后一个仍然是当前时刻的，在线不是预测，而是实时更新

    # online

    num_train = train_x.shape[0] - train_size

    r2_list_1 = []

    r2_list_2 = []

    r2_list_3 = []

    r2_list_4 = []

    r2_list_5 = []

    r2_list_6 = []

    # r2_list_7 = []

    # r2_list_8 = []

    # r2_list_9 = []

  

    accuracy_list_1 = []

    accuracy_list_2 = []

    accuracy_list_3 = []

    accuracy_list_4 = []

    accuracy_list_5 = []

    accuracy_list_6 = []

    # accuracy_list_7 = []

    # accuracy_list_8 = []

    # accuracy_list_9 = []

    trainloss_list = []

    Running_time_list = []

    # plt.ion()

    # plt.figure(1)

    number = batchsize + lags

    # tick_labels = []

    y_calc_1 = []

    y_calc_2 = []

    y_calc_3 = []

    y_calc_4 = []

    y_calc_5 = []

    y_calc_6 = []

    # y_calc_7 = []

    # y_calc_8 = []

    # y_calc_9 = []

    y_ori_1 = []

    y_ori_2 = []

    y_ori_3 = []

    y_ori_4 = []

    y_ori_5 = []

    y_ori_6 = []

    # y_ori_7 = []

    # y_ori_8 = []

    # y_ori_9 = []

    M0=np.zeros((num_train-lags,features,features))

    M1=np.zeros((num_train-lags,features,features))

    M2=np.zeros((num_train-lags,features,features))

    # online

    for i in range(lags, num_train):

    # for i in range(1):

        start = time.time()

        train_loss_epoch = []

        righth = i + train_size

        train_data = train_x[i:righth,:]

        # start = time.time()

        # nn, train_loss, train_loss_epoch = train(train_data, batch_size=batchsize, epochs=epochs, lags=lags)

        # nn = BP(batchsize = batchsize, lags = lags, features = features, learning_rate = learning_rate)

        nn, train_loss_epoch = train(train_data, batch_size=batchsize, epochs=epochs, lags=lags)

        end = time.time()

        # print('Running time: %s Seconds'%(end-start))

        Running_time_list.append(end - start)

        # trainloss = np.mean(train_loss_epoch)

        # print(trainloss)

        # 可能存在batchsize分配不均，导致后几个没有训练到的问题

        # test_x = train_x[righth - batchsize - lags:righth,:]

        # test_x = train_data[-number:,:]

        test_x = train_data[number:,:]

        # test_x = train_data[:-number,:]

        test_x = train_data

        accuracy, res, testy,r2,z2 = test_online(test_x, lags=lags)

        # accuracy, res, testy = test_online(test_data, lags=lags)

        # print(accuracy)

        r2_list_1.append(r2[0])

        r2_list_2.append(r2[1])

        r2_list_3.append(r2[2])

        r2_list_4.append(r2[3])

        r2_list_5.append(r2[4])

        r2_list_6.append(r2[5])

        # r2_list_7.append(r2[6])

        # r2_list_8.append(r2[7])

        # r2_list_9.append(r2[8])

        # accuracy_list.append(accuracy)

        accuracy_list_1 .append(accuracy[0])

        accuracy_list_2 .append(accuracy[1])

        accuracy_list_3 .append(accuracy[2])

        accuracy_list_4 .append(accuracy[3])

        accuracy_list_5 .append(accuracy[4])

        accuracy_list_6 .append(accuracy[5])

        # accuracy_list_7 .append(accuracy[6])

        # accuracy_list_8 .append(accuracy[7])

        # accuracy_list_9 .append(accuracy[8])

        trainloss = np.mean(train_loss_epoch)

        # print(trainloss)

        trainloss_list.append(trainloss)

        # xax = righth - 1

        # tick_labels.append(xax)

        y_calc_1.append(res[-1,0])

        y_ori_1.append(testy[-1:,0])

        y_calc_2.append(res[-1,1])

        y_ori_2.append(testy[-1:,1])

        y_calc_3.append(res[-1,2])

        y_ori_3.append(testy[-1:,2])

        y_calc_4.append(res[-1,3])

        y_ori_4.append(testy[-1:,3])

        y_calc_5.append(res[-1,4])

        y_ori_5.append(testy[-1:,4])

        y_calc_6.append(res[-1,5])

        y_ori_6.append(testy[-1:,5])

        # y_calc_7.append(res[-1,6])

        # y_ori_7.append(testy[-1:,6])

        # y_calc_8.append(res[-1,7])

        # y_ori_8.append(testy[-1:,7])

        # y_calc_9.append(res[-1,8])

        # y_ori_9.append(testy[-1:,8])

  

        # z=z2

        # z2=z2[-1,:].reshape(features*(lags+1),features)

        # M0[i-2,:,:]=z2[:6,:]

        # M1[i-2,:,:]=z2[6:12,:]

        # M2[i-2,:,:]=z2[12:18,:]

  

        # Data_ori={'one':y_ori_1, 'two':y_ori_2, 'three':y_ori_3,'four':y_ori_4, 'five':y_ori_5, 'six':y_ori_6}

        # Data_pred={'one':y_calc_1, 'two':y_calc_2, 'three':y_calc_3,'four':y_calc_4, 'five':y_calc_5, 'six':y_calc_6}

        # # Data_ori={'one':y_ori_1, 'two':y_ori_2, 'three':y_ori_3,'four':y_ori_4}

        # # Data_pred={'one':y_calc_1, 'two':y_calc_2, 'three':y_calc_3,'four':y_calc_4}

        # Save_to_Csv(data = Data_ori, file_name = 'Data_ori_100', Save_format = 'csv',Save_type = 'col')

        # Save_to_Csv(data = Data_pred, file_name = 'Data_pred_100', Save_format = 'csv',Save_type = 'col')

        # 'seven':y_ori_7, 'eight':y_ori_8, 'nine':y_ori_9

        # ,'seven':y_calc_7, 'eight':y_calc_8, 'nine':y_calc_9

    # plt.figure()

    # plt.subplot(231)

    # plt.plot(y_calc_1, c='red', marker='o', ms=1, alpha=0.75, label='sim')

    # plt.plot(y_ori_1, c='blue', marker='*', ms=1, alpha=0.75, label='real')

    # plt.subplot(232)

    # plt.plot(y_calc_2, c='red', marker='o', ms=1, alpha=0.75, label='sim')

    # plt.plot(y_ori_2, c='blue', marker='*', ms=1, alpha=0.75, label='real')

    # plt.subplot(233)

    # plt.plot(y_calc_3, c='red', marker='o', ms=1, alpha=0.75, label='sim')

    # plt.plot(y_ori_3, c='blue', marker='*', ms=1, alpha=0.75, label='real')

    # plt.subplot(234)

    # plt.plot(y_calc_4, c='red', marker='o', ms=1, alpha=0.75, label='sim')

    # plt.plot(y_ori_4, c='blue', marker='*', ms=1, alpha=0.75, label='real')

    # plt.subplot(235)

    # plt.plot(y_calc_5, c='red', marker='o', ms=1, alpha=0.75, label='sim')

    # plt.plot(y_ori_5, c='blue', marker='*', ms=1, alpha=0.75, label='real')

    # plt.subplot(236)

    # plt.plot(y_calc_6, c='red', marker='o', ms=1, alpha=0.75, label='sim')

    # plt.plot(y_ori_6, c='blue', marker='*', ms=1, alpha=0.75, label='real')

    # plt.grid(axis='y')

    # plt.legend()

    # plt.show()

    # # plt.figure()

    # # plt.plot(accuracy_list)

    # # plt.grid(axis='y')

    # # plt.legend()

    # # plt.show()

    # plt.figure()

    # plt.plot(trainloss_list)

    # plt.grid(axis='y')

    # plt.legend()

    # plt.show()

    print('准确率：')

    print(np.mean(accuracy_list_1))

    print(np.mean(accuracy_list_2))

    print(np.mean(accuracy_list_3))

    print(np.mean(accuracy_list_4))

    print(np.mean(accuracy_list_5))

    print(np.mean(accuracy_list_6))

    # print(np.mean(accuracy_list_7))

    # print(np.mean(accuracy_list_8))

    # print(np.mean(accuracy_list_9))

    print('Running time: %s Seconds'%(np.mean(Running_time_list)))

    print('训练误差：')

    print(np.mean(trainloss_list))

    print('r2：')

    print(np.mean(r2_list_1))

    print(np.mean(r2_list_2))

    print(np.mean(r2_list_3))

    print(np.mean(r2_list_4))

    print(np.mean(r2_list_5))

    print(np.mean(r2_list_6))

    # print(np.mean(r2_list_7))

    # print(np.mean(r2_list_8))

    # print(np.mean(r2_list_9))

    # z2.to_csv('C:/Users/zhouw/Desktop/z2.csv')

  

    # np.savetxt("C:/Users/zhouw/Desktop/z2.txt",z)

  

    # 其他参数->BV

    r0_V=np.zeros((num_train-lags,features))

    r1_V=np.zeros((num_train-lags,features))

    r2_V=np.zeros((num_train-lags,features))

    r0_I=np.zeros((num_train-lags,features))

    r1_I=np.zeros((num_train-lags,features))

    r2_I=np.zeros((num_train-lags,features))

    for j in range(num_train-lags):

        r0_V[j,:]=M0[j,5,:]

        r1_V[j,:]=M1[j,5,:]

        r2_V[j,:]=M2[j,5,:]

        r0_I[j,:]=M0[j,1,:]

        r1_I[j,:]=M1[j,1,:]

        r2_I[j,:]=M2[j,1,:]

    # np.savetxt("C:/Users/zhouw/Desktop/r0_V.txt",r0_V)

    # np.savetxt("C:/Users/zhouw/Desktop/r1_V.txt",r1_V)

    # np.savetxt("C:/Users/zhouw/Desktop/r2_V.txt",r2_V)

    # np.savetxt("C:/Users/zhouw/Desktop/r0_I.txt",r0_I)

    # np.savetxt("C:/Users/zhouw/Desktop/r1_I.txt",r1_I)

    # np.savetxt("C:/Users/zhouw/Desktop/r2_I.txt",r2_I)

  

    # import matplotlib.pyplot as plt

    # fig1, axes = plt.subplots(nrows=1,ncols=4, dpi=120,figsize=(3,12))

    # for i,ax in enumerate(axes.flatten()):

    #     data=r0[:,i]

    #     ax.plot(data,color='red',linewidth=1)

    # # Decorations

    #     # ax.set_title(df.columns[i])

    #     ax.xaxis.set_ticks_position('none')

    #     ax.yaxis.set_ticks_position('none')

    #     ax.spines["top"].set_alpha(0)

    #     ax.tick_params(labelsize=6)

    # plt.tight_layout()

  

    # fig2, axes = plt.subplots(nrows=1,ncols=4, dpi=120,figsize=(3,12))

    # for i,ax in enumerate(axes.flatten()):

    #     data=r1[:,i]

    #     ax.plot(data,color='red',linewidth=1)

    # # Decorations

    #     # ax.set_title(df.columns[i])

    #     ax.xaxis.set_ticks_position('none')

    #     ax.yaxis.set_ticks_position('none')

    #     ax.spines["top"].set_alpha(0)

    #     ax.tick_params(labelsize=6)

    # plt.tight_layout()

  

    # fig3, axes = plt.subplots(nrows=1,ncols=4, dpi=120,figsize=(3,12))

    # for i,ax in enumerate(axes.flatten()):

    #     data=r2[:,i]

    #     ax.plot(data,color='red',linewidth=1)

    # # Decorations

    #     # ax.set_title(df.columns[i])

    #     ax.xaxis.set_ticks_position('none')

    #     ax.yaxis.set_ticks_position('none')

    #     ax.spines["top"].set_alpha(0)

    #     ax.tick_params(labelsize=6)

    # plt.tight_layout()

  

    # # accuracy_list = pd.DataFrame(accuracy_list)

    # # accuracy_list.to_csv('accuracy7_list45000-47batchszie2lags1transize100_features6.csv')

    # print('complete')

    # y_calc = pd.DataFrame(y_calc)

    # y_ori = pd.DataFrame(y_ori)

    # y_ori.to_csv("y_ori.csv")

    # y_calc.to_csv("y_calc.csv")

  

    # offline

    # start = time.time()

    # nn, train_loss, train_loss_epoch = train(train_x=train_x, batch_size=batchsize, epochs=epochs, lags=lags)

    # accuracy = test(test_x, test_y, batch_size=batchsize, lags=lags)

    # end = time.time()

    # print('Running time: %s Seconds'%(end-start))

    # *****************************************

    # train_loss = pd.DataFrame(train_loss)

    # train_loss.to_csv("train_loss.csv")

  

    # train_loss_epoch = pd.DataFrame(train_loss_epoch)

    # train_loss_epoch.to_csv("trainlossepoch50_batchsize2_lags2_45000_48.csv")

  

    # df_s = pd.DataFrame(df)

    # df_s.to_csv('df.csv')
~~~

# 信号分析代码

Matlab

D:\工作\课题\采购试验台\转子试验台台架\试验\数据\采样频率分析

~~~
load B1500r10kL14.mat

%% 参数设置

data = B1500r10kL14(:,4); % 927372×1的数据

segment_size = 1000; % 每段数据点数

total_samples = length(data);

num_segments = floor(total_samples / segment_size); % 计算完整段数

%% 初始化参数存储矩阵

% 每行对应一个数据段，每列对应一个时域参数

params = zeros(num_segments, 9);

param_names = {'波形因子', '裕度因子', '峭度', '偏度', '均方根值', '标准差', '均值', '峰值', '峰峰值'};

%% 逐段计算时域参数

for i = 1:num_segments

% 提取当前数据段

start_idx = (i-1)*segment_size + 1;

end_idx = i*segment_size;

segment = data(start_idx:end_idx);

% 计算基本统计量

mean_val = mean(segment); % 均值

std_val = std(segment); % 标准差

rms_val = sqrt(mean(segment.^2)); % 均方根值

max_val = max(segment); % 最大值

min_val = min(segment); % 最小值

peak_val = max(abs(segment)); % 峰值

peak_to_peak = max_val - min_val; % 峰峰值

% 计算偏度（三阶中心矩）

skewness_val = mean((segment - mean_val).^3) / (std_val^3);

% 计算峭度（四阶中心矩）

kurtosis_val = mean((segment - mean_val).^4) / (std_val^4);

% 计算波形因子（Form Factor）

% 波形因子 = RMS / 绝对均值

abs_mean = mean(abs(segment));

form_factor = rms_val / abs_mean;

% 计算裕度因子（Crest Factor）

% 裕度因子 = 峰值 / RMS

crest_factor = peak_val / rms_val;

% 存储计算结果

params(i, :) = [form_factor, crest_factor, kurtosis_val, skewness_val, ...

rms_val, std_val, mean_val, peak_val, peak_to_peak];

% 显示进度（可选）

if mod(i, 100) == 0

fprintf('已处理 %d/%d 段数据\n', i, num_segments);

end

end

%% 输出结果分析

fprintf('\n========== 处理完成 ==========\n');

fprintf('总数据点数: %d\n', total_samples);

fprintf('分段大小: %d\n', segment_size);

fprintf('完整段数: %d\n', num_segments);

fprintf('剩余数据点数: %d\n', mod(total_samples, segment_size));

fprintf('\n计算的时域参数:\n');

for i = 1:length(param_names)

fprintf('%d. %s\n', i, param_names{i});

end

%% 保存结果到文件（可选）

% save('time_domain_parameters.mat', 'params', 'param_names');

%% 可视化部分结果（可选）

figure('Position', [100, 100, 1200, 800]);

% 绘制均值变化

subplot(3, 3, 1);

plot(params(:, 7));

title('均值变化');

xlabel('段序号');

ylabel('均值');

grid on;

% 绘制均方根值变化

subplot(3, 3, 2);

plot(params(:, 5));

title('均方根值变化');

xlabel('段序号');

ylabel('RMS');

grid on;

% 绘制标准差变化

subplot(3, 3, 3);

plot(params(:, 6));

title('标准差变化');

xlabel('段序号');

ylabel('标准差');

grid on;

% 绘制峭度变化

subplot(3, 3, 4);

plot(params(:, 3));

title('峭度变化');

xlabel('段序号');

ylabel('峭度');

grid on;

% 绘制偏度变化

subplot(3, 3, 5);

plot(params(:, 4));

title('偏度变化');

xlabel('段序号');

ylabel('偏度');

grid on;

% 绘制波形因子变化

subplot(3, 3, 6);

plot(params(:, 1));

title('波形因子变化');

xlabel('段序号');

ylabel('波形因子');

grid on;

% 绘制裕度因子变化

subplot(3, 3, 7);

plot(params(:, 2));

title('裕度因子变化');

xlabel('段序号');

ylabel('裕度因子');

grid on;

% 绘制峰值变化

subplot(3, 3, 8);

plot(params(:, 8));

title('峰值变化');

xlabel('段序号');

ylabel('峰值');

grid on;

% 绘制峰峰值变化

subplot(3, 3, 9);

plot(params(:, 9));

title('峰峰值变化');

xlabel('段序号');

ylabel('峰峰值');

grid on;

%% 统计参数的基本统计信息（可选）

fprintf('\n========== 参数统计信息 ==========\n');

for i = 1:length(param_names)

fprintf('\n%s:\n', param_names{i});

fprintf(' 平均值: %.4f\n', mean(params(:, i)));

fprintf(' 标准差: %.4f\n', std(params(:, i)));

fprintf(' 最小值: %.4f\n', min(params(:, i)));

fprintf(' 最大值: %.4f\n', max(params(:, i)));

end

%% 处理剩余数据（如果需要）

if mod(total_samples, segment_size) > 0

remaining_data = data(num_segments*segment_size+1:end);

fprintf('\n剩余 %d 个数据点未处理\n', length(remaining_data));

% 这里可以添加对剩余数据的处理

end
~~~
